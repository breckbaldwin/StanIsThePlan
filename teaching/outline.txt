Joke: I can model and drink beer, I cannot program and drink beer. Modeling is like writing, you are working on a narrative for your data/problem. The beer helps with creativity that you clean up later. 

Joke: Circle line. 

Deep learning comparision: Deep learning works by training a model and deciding if it did anything useful by evaluating on held out data/xval. Bayesian modeling works by creating and validating a model (with or without looking at data) and once the model is acceptable you accept the reality of what you learn with data. It may not be useful but it is correct. It is "throw spagetti against the wall and see what sticks approach vs eat the spagetti you cooked no matter what". Tyranicaly Toddler vs Adult. Example is to model a coin, bias the data a bit and see the differences in 

- Assume that some people learn best by doing and watching things run. 
Who this class is for:

Not statisticians, for programmers. Let us do a feature matrix

Great programmers communicate well via source code/documentation, great statisticians work in literally Greek where succinctness is valued over understandability. 

Statiticians are valued for being clever, programmers for building good stuff. 

Programmers value longevity and future growth, staticians value fame

Programmers know they are hobbits, statiticians want to be eleves. 

Signs of a good programmer: Lower educational attainment, clarity trumps cleverness/succintness, builds things

Signs of a good statistician: Higher educaitonal attainment, writes in literally greek--secret society levels of hidden meaning, less is more, hires someone to assemble ikea furniture. 





- cmdstan, simple program
  - install
  - run
  - look at output
- Overview of RStudio interface
  - Helloworld

-Motivating example
  - Why Bayesian Modeling?
  - Jagged Profile--30% around mean value. 2% of pilots were avg on 4 dimensions, 0 on all.
  - simulate pilot data in R or in generated quantities.
  - Expand 'middle' to 90%, only 50% fit in that zone.
  - Very little data is actually "big data". 
  
- data -> model=dist1...distn. 

prediction 

given data + param(fn training data) 
  
-Play with distributions
  show identity
  concentration 20x data
  alternate scales log
  averaging 0,42
  bimodal 0,42

-Sampling
  Grid--bayes rule
    - break grid with dimensionality
  Met Hastings
  Stan version
    -target += 
- Stan Hello World. 
  - What runs when.
- Modeling
  - A/B test case
  - A/B with hier
  - Strong model, golf.
  - Prediction and AI <need help with prediciton> 
  - Continuous vs discrete
- Validation
  - Diagnostics
- Bayesian workflow: How to encode your problem in Stan
  - No promises
  -
  
  Sources of non-concentrated dists. 
    - Not much data
    - broad values for data we see
    - ??
  
Sociology of Stan
  -Fellow Bayesians are assholes.
  -

  - 
  
Pooling is a wet blanket: Alan. 
  
Sales pitch: A fully fleshed example
  A/B testing example
  Naive example of straight counting. How can this be improved?
    Sentivity to what we know about click through rates generally
    Sensivity to the amount of data we have. With huge data just count.
    We can do well with less data,the methods are:
      priors: Don't waste effort figuring out that 10% is the expected rate. ? how much do you gain from strong priors?
      It reduces your data consumption by order of maginitude. 
    Write program to add the same data to a non-pooled and a partially pooled stan progam. 
    
    clicks per 500 views
    
    A 1 B 0

    +/- prior that is interesting
    +/- partial pooling
    visualize how it looks as you add data
    
    blue vs purple +100k
    
        
    
  One Page w/10% rate, what do you think?
    Is this good, bad, normal. 
    Ask what is a good click through rate? 
      Introduce notion of prior
      W/o prior you may have to do lots more data collection
        size of data matters whether naive assumpotiosn or priors
    We can do better with both priors and similar alternatives that we want the best of.
    
    

Introduction of scope
  - What we will cover. 
  - Getting started with Rstudio Cloud
    - Credentials, github/google/facebook to url https://rstudio.cloud/project/405221
    - Copies my home dir to your own instance. All relevant class materials should be there in Lecture0 folder. Click to the right of the folder icon, file name....
    - "save a permament copy" if you want to come back to where you left off. 
    - Compile/run simple Stan program

Sampling demo--playing with distributions and sampling. 
  - single value normal
      - we get probability distribution for possible values for the_answer
  - single value lognormal--doesn't work great, 20 mean for 42 answer
      *exp(mean(dist)) = 20
  - 20x single value lognormal works fine. 

  - add data that supports a distribution
  - Do it by generating from a distribution, this is a step in Bayesian work flow. 
  - Discuss bad fits and the signs of them in diagnostics. 
  - 
#Discrete vs continuous data

#How sampling works
  - met hastings
  - Stan
From "The end of Average"
p 11 "Averages have thier place. If you're comparing two differnent groups of people, liek comparing...., as opposed to comapring two individuals from each of those groups."

"(averages) create the illusion of knowledge, when in fact the average disguises what is most important about an individual."

"

Averages drive us towards beige/grey if the extremes are colors. 
SHow the paint trick of mixing paint stipes and unmixxing them to original.https://theawesomer.com/color-unmixing-machine/525432/

Art idea: take the Mona Lisa, get average of all colors and paint it. 
Get iconic images and turn it into a guessing game what the source was. Get Karen to do the work/PR. 
Make them puzzles as to what exactly has been averaged over. Round image. Title with Stan program. Recreate orignial with more and more complete representation. 
*Average*

If we are to avoid the tyranny of average but do useful mathematical reasoning we need posteriors. 


Why models matter: From "End of Average" Todd Rose. P 64. Overall faster typists make fewer errors. Average based analysis would lead one to think that a typist should type faster to reduce errors when in fact the opposite is true. The error is complete pooling, partial pooling would be better. Not sure????

An average value will in general do better than pick at random even if the average runs roughshod over differences. There has to be a tradeoff point. 

# Naive Bayes
You can ignore all the fancy stats that generated the posterior and sample directly. The goal is the posterior, means etc...Any means to the means. 



